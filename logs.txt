
==> Audit <==
┌─────────┬───────────────────────────────┬──────────┬────────────────┬─────────┬─────────────────────┬─────────────────────┐
│ COMMAND │             ARGS              │ PROFILE  │      USER      │ VERSION │     START TIME      │      END TIME       │
├─────────┼───────────────────────────────┼──────────┼────────────────┼─────────┼─────────────────────┼─────────────────────┤
│ start   │                               │ minikube │ MRMAN-PC\mrman │ v1.37.0 │ 07 Oct 25 15:36 GMT │ 07 Oct 25 15:42 GMT │
│ service │ devops-kubernetes-api-service │ minikube │ MRMAN-PC\mrman │ v1.37.0 │ 07 Oct 25 15:47 GMT │                     │
│ service │ kubernetes-api-service        │ minikube │ MRMAN-PC\mrman │ v1.37.0 │ 07 Oct 25 15:48 GMT │                     │
│ service │ list                          │ minikube │ MRMAN-PC\mrman │ v1.37.0 │ 07 Oct 25 15:48 GMT │ 07 Oct 25 15:48 GMT │
│ service │ devops-kubernetes-api-service │ minikube │ MRMAN-PC\mrman │ v1.37.0 │ 07 Oct 25 15:55 GMT │                     │
└─────────┴───────────────────────────────┴──────────┴────────────────┴─────────┴─────────────────────┴─────────────────────┘


==> Last Start <==
Log file created at: 2025/10/07 15:36:32
Running on machine: mrman-pc
Binary: Built with gc go1.24.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1007 15:36:32.604757   21292 out.go:360] Setting OutFile to fd 480 ...
I1007 15:36:32.606333   21292 out.go:413] isatty.IsTerminal(480) = true
I1007 15:36:32.606333   21292 out.go:374] Setting ErrFile to fd 480...
I1007 15:36:32.607344   21292 out.go:413] isatty.IsTerminal(480) = true
W1007 15:36:32.652675   21292 root.go:314] Error reading config file at C:\Users\mrman\.minikube\config\config.json: open C:\Users\mrman\.minikube\config\config.json: The system cannot find the file specified.
I1007 15:36:32.675247   21292 out.go:368] Setting JSON to false
I1007 15:36:32.680253   21292 start.go:130] hostinfo: {"hostname":"mrman-pc","uptime":14630,"bootTime":1759836761,"procs":316,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.6725 Build 26100.6725","kernelVersion":"10.0.26100.6725 Build 26100.6725","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"9744363f-1a0f-41bc-afc9-0c2822895758"}
W1007 15:36:32.680832   21292 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1007 15:36:32.690105   21292 out.go:179] 😄  minikube v1.37.0 on Microsoft Windows 11 Pro 10.0.26100.6725 Build 26100.6725
I1007 15:36:32.697483   21292 notify.go:220] Checking for updates...
W1007 15:36:32.698638   21292 preload.go:293] Failed to list preload files: open C:\Users\mrman\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I1007 15:36:32.713030   21292 driver.go:421] Setting default libvirt URI to qemu:///system
I1007 15:36:32.714190   21292 global.go:112] Querying for installed drivers using PATH=C:\Users\mrman\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\local\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\bin;C:\Users\mrman\bin;C:\Program Files\Python313\Scripts;C:\Program Files\Python313;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\nodejs;C:\Program Files\Git\cmd;C:\Program Files\PowerShell\7;C:\Program Files\Kubernetes\Minikube;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Python313\Scripts;C:\Program Files\Python313;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\nodejs;C:\Program Files\Git\cmd;C:\Program Files\PowerShell\7;C:\Users\mrman\.console-ninja\.bin;C:\Users\mrman\AppData\Local\Programs\oh-my-posh\bin;C:\Users\mrman\AppData\Local\Microsoft\WindowsApps;C:\Users\mrman\AppData\Roaming\npm;C:\Users\mrman\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\mrman\AppData\Local\Programs\mongosh;C:\Program Files\Kubernetes;C:\Program Files\Kubernetes;C:\Users\mrman\.console-ninja\.bin;C:\Program Files\Git\usr\bin\vendor_perl;C:\Program Files\Git\usr\bin\core_perl;C:\Program Files\Kubernetes;C:\Program Files\Kubernetes;C:\;C:\Program Files\Git\Program Files\Kubernetes;C:\Program Files\Kubernetes\Minikube
I1007 15:36:32.863280   21292 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1007 15:36:32.928551   21292 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1007 15:36:33.397596   21292 docker.go:123] docker version: linux-28.3.2:Docker Desktop 4.43.2 (199162)
I1007 15:36:33.425388   21292 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1007 15:36:34.838881   21292 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.4134929s)
I1007 15:36:34.847707   21292 info.go:266] docker info: {ID:4884fd32-431d-44c3-b3c8-f7ffad56d5c7 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:73 OomKillDisable:false NGoroutines:119 SystemTime:2025-10-07 15:36:34.791817068 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:7218630656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I1007 15:36:34.848292   21292 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1007 15:36:34.915682   21292 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1007 15:36:34.915682   21292 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1007 15:36:40.224396   21292 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:Hyper-V requires Administrator privileges Reason: Fix:Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode. Doc: Version:}
I1007 15:36:40.271485   21292 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1007 15:36:40.272487   21292 driver.go:343] not recommending "ssh" due to default: false
I1007 15:36:40.272487   21292 driver.go:338] not recommending "hyperv" due to health: Hyper-V requires Administrator privileges
I1007 15:36:40.272487   21292 driver.go:378] Picked: docker
I1007 15:36:40.274991   21292 driver.go:379] Alternatives: [ssh]
I1007 15:36:40.274991   21292 driver.go:380] Rejects: [virtualbox vmware podman hyperv qemu2]
I1007 15:36:40.276687   21292 out.go:179] ✨  Automatically selected the docker driver
I1007 15:36:40.283570   21292 start.go:304] selected driver: docker
I1007 15:36:40.283570   21292 start.go:918] validating driver "docker" against <nil>
I1007 15:36:40.283570   21292 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1007 15:36:40.338881   21292 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1007 15:36:40.918458   21292 info.go:266] docker info: {ID:4884fd32-431d-44c3-b3c8-f7ffad56d5c7 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:73 OomKillDisable:false NGoroutines:119 SystemTime:2025-10-07 15:36:40.886273335 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:7218630656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I1007 15:36:40.920641   21292 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1007 15:36:41.019972   21292 start_flags.go:410] Using suggested 3500MB memory alloc based on sys=14237MB, container=6884MB
I1007 15:36:41.021615   21292 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1007 15:36:41.024789   21292 out.go:179] 📌  Using Docker Desktop driver with root privileges
I1007 15:36:41.030570   21292 cni.go:84] Creating CNI manager for ""
I1007 15:36:41.032213   21292 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1007 15:36:41.032213   21292 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1007 15:36:41.032735   21292 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1007 15:36:41.036571   21292 out.go:179] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1007 15:36:41.041312   21292 cache.go:123] Beginning downloading kic base image for docker with docker
I1007 15:36:41.044938   21292 out.go:179] 🚜  Pulling base image v0.0.48 ...
I1007 15:36:41.049743   21292 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1007 15:36:41.049743   21292 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1007 15:36:41.252958   21292 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1007 15:36:41.254602   21292 localpath.go:148] windows sanitize: C:\Users\mrman\.minikube\cache\kic\amd64\kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\mrman\.minikube\cache\kic\amd64\kicbase_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1007 15:36:41.256260   21292 localpath.go:148] windows sanitize: C:\Users\mrman\.minikube\cache\kic\amd64\kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\mrman\.minikube\cache\kic\amd64\kicbase_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1007 15:36:41.256832   21292 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1007 15:36:41.257932   21292 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1007 15:36:41.298005   21292 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1007 15:36:41.298005   21292 cache.go:58] Caching tarball of preloaded images
I1007 15:36:41.299156   21292 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1007 15:36:41.302562   21292 out.go:179] 💾  Downloading Kubernetes v1.34.0 preload ...
I1007 15:36:41.306524   21292 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1007 15:36:41.992935   21292 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> C:\Users\mrman\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1007 15:38:45.682958   21292 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1007 15:38:45.688730   21292 preload.go:254] verifying checksum of C:\Users\mrman\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1007 15:38:46.608297   21292 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1007 15:38:46.614536   21292 profile.go:143] Saving config to C:\Users\mrman\.minikube\profiles\minikube\config.json ...
I1007 15:38:46.615539   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\profiles\minikube\config.json: {Name:mk521691c461a201622120cd9bfab34e875d2828 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:39:48.470695   21292 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1007 15:39:48.470695   21292 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1007 15:39:48.471822   21292 localpath.go:148] windows sanitize: C:\Users\mrman\.minikube\cache\kic\amd64\kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\mrman\.minikube\cache\kic\amd64\kicbase_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1007 15:40:47.646859   21292 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1007 15:40:47.648029   21292 cache.go:232] Successfully downloaded all kic artifacts
I1007 15:40:47.655027   21292 start.go:360] acquireMachinesLock for minikube: {Name:mk77209e7af91c62913152d980931df7b490701a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1007 15:40:47.655027   21292 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1007 15:40:47.657302   21292 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1007 15:40:47.657302   21292 start.go:125] createHost starting for "" (driver="docker")
I1007 15:40:47.661340   21292 out.go:252] 🔥  Creating docker container (CPUs=2, Memory=3500MB) ...
I1007 15:40:47.669268   21292 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1007 15:40:47.669268   21292 client.go:168] LocalClient.Create starting
I1007 15:40:47.672717   21292 main.go:141] libmachine: Creating CA: C:\Users\mrman\.minikube\certs\ca.pem
I1007 15:40:48.011138   21292 main.go:141] libmachine: Creating client certificate: C:\Users\mrman\.minikube\certs\cert.pem
I1007 15:40:48.118535   21292 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1007 15:40:48.233550   21292 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1007 15:40:48.258000   21292 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1007 15:40:48.258000   21292 cli_runner.go:164] Run: docker network inspect minikube
W1007 15:40:48.360512   21292 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1007 15:40:48.360512   21292 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1007 15:40:48.360512   21292 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1007 15:40:48.382845   21292 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1007 15:40:48.588045   21292 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001611830}
I1007 15:40:48.588639   21292 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1007 15:40:48.620216   21292 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1007 15:40:49.001723   21292 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1007 15:40:49.002310   21292 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1007 15:40:49.056678   21292 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1007 15:40:49.227833   21292 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1007 15:40:49.370048   21292 oci.go:103] Successfully created a docker volume minikube
I1007 15:40:49.413876   21292 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1007 15:40:52.571691   21292 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib: (3.1578141s)
I1007 15:40:52.571691   21292 oci.go:107] Successfully prepared a docker volume minikube
I1007 15:40:52.572820   21292 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1007 15:40:52.573412   21292 kic.go:194] Starting extracting preloaded images to volume ...
I1007 15:40:52.608558   21292 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\mrman\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1007 15:41:11.437262   21292 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\mrman\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (18.8280649s)
I1007 15:41:11.437262   21292 kic.go:203] duration metric: took 18.8644419s to extract preloaded images to volume ...
I1007 15:41:11.473663   21292 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1007 15:41:12.827798   21292 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.3541349s)
I1007 15:41:12.828332   21292 info.go:266] docker info: {ID:4884fd32-431d-44c3-b3c8-f7ffad56d5c7 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:76 OomKillDisable:false NGoroutines:123 SystemTime:2025-10-07 15:41:12.789545021 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:7218630656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I1007 15:41:12.853788   21292 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1007 15:41:13.556960   21292 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3500mb --memory-swap=3500mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1007 15:41:14.860686   21292 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3500mb --memory-swap=3500mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1: (1.3031501s)
I1007 15:41:14.888554   21292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1007 15:41:15.080361   21292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1007 15:41:15.276460   21292 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1007 15:41:15.673773   21292 oci.go:144] the created container "minikube" has a running status.
I1007 15:41:15.674385   21292 kic.go:225] Creating ssh key for kic: C:\Users\mrman\.minikube\machines\minikube\id_rsa...
I1007 15:41:15.992927   21292 kic_runner.go:191] docker (temp): C:\Users\mrman\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1007 15:41:16.315439   21292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1007 15:41:16.545377   21292 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1007 15:41:16.545377   21292 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1007 15:41:16.840248   21292 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\mrman\.minikube\machines\minikube\id_rsa...
I1007 15:41:18.639126   21292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1007 15:41:18.758479   21292 machine.go:93] provisionDockerMachine start ...
I1007 15:41:18.800657   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:18.919336   21292 main.go:141] libmachine: Using SSH client type: native
I1007 15:41:18.941230   21292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6617c0] 0x664300 <nil>  [] 0s} 127.0.0.1 52128 <nil> <nil>}
I1007 15:41:18.941230   21292 main.go:141] libmachine: About to run SSH command:
hostname
I1007 15:41:19.150948   21292 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1007 15:41:19.152615   21292 ubuntu.go:182] provisioning hostname "minikube"
I1007 15:41:19.177107   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:19.279968   21292 main.go:141] libmachine: Using SSH client type: native
I1007 15:41:19.280562   21292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6617c0] 0x664300 <nil>  [] 0s} 127.0.0.1 52128 <nil> <nil>}
I1007 15:41:19.280562   21292 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1007 15:41:19.510826   21292 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1007 15:41:19.538189   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:19.653695   21292 main.go:141] libmachine: Using SSH client type: native
I1007 15:41:19.654240   21292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6617c0] 0x664300 <nil>  [] 0s} 127.0.0.1 52128 <nil> <nil>}
I1007 15:41:19.654240   21292 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1007 15:41:19.850714   21292 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1007 15:41:19.851273   21292 ubuntu.go:188] set auth options {CertDir:C:\Users\mrman\.minikube CaCertPath:C:\Users\mrman\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\mrman\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\mrman\.minikube\machines\server.pem ServerKeyPath:C:\Users\mrman\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\mrman\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\mrman\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\mrman\.minikube}
I1007 15:41:19.851273   21292 ubuntu.go:190] setting up certificates
I1007 15:41:19.851273   21292 provision.go:84] configureAuth start
I1007 15:41:19.880551   21292 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1007 15:41:19.991556   21292 provision.go:143] copyHostCerts
I1007 15:41:19.993856   21292 exec_runner.go:151] cp: C:\Users\mrman\.minikube\certs\ca.pem --> C:\Users\mrman\.minikube/ca.pem (1074 bytes)
I1007 15:41:19.995714   21292 exec_runner.go:151] cp: C:\Users\mrman\.minikube\certs\cert.pem --> C:\Users\mrman\.minikube/cert.pem (1119 bytes)
I1007 15:41:19.998000   21292 exec_runner.go:151] cp: C:\Users\mrman\.minikube\certs\key.pem --> C:\Users\mrman\.minikube/key.pem (1679 bytes)
I1007 15:41:19.999730   21292 provision.go:117] generating server cert: C:\Users\mrman\.minikube\machines\server.pem ca-key=C:\Users\mrman\.minikube\certs\ca.pem private-key=C:\Users\mrman\.minikube\certs\ca-key.pem org=mrman.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1007 15:41:20.452040   21292 provision.go:177] copyRemoteCerts
I1007 15:41:20.471613   21292 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1007 15:41:20.497833   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:20.605057   21292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52128 SSHKeyPath:C:\Users\mrman\.minikube\machines\minikube\id_rsa Username:docker}
I1007 15:41:20.742726   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1007 15:41:20.810289   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1007 15:41:20.875802   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1007 15:41:20.945711   21292 provision.go:87] duration metric: took 1.0905336s to configureAuth
I1007 15:41:20.945711   21292 ubuntu.go:206] setting minikube options for container-runtime
I1007 15:41:20.947884   21292 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1007 15:41:20.972571   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:21.106593   21292 main.go:141] libmachine: Using SSH client type: native
I1007 15:41:21.108567   21292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6617c0] 0x664300 <nil>  [] 0s} 127.0.0.1 52128 <nil> <nil>}
I1007 15:41:21.108567   21292 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1007 15:41:21.319400   21292 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1007 15:41:21.319400   21292 ubuntu.go:71] root file system type: overlay
I1007 15:41:21.320505   21292 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1007 15:41:21.344324   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:21.457280   21292 main.go:141] libmachine: Using SSH client type: native
I1007 15:41:21.457832   21292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6617c0] 0x664300 <nil>  [] 0s} 127.0.0.1 52128 <nil> <nil>}
I1007 15:41:21.457832   21292 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1007 15:41:21.679312   21292 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1007 15:41:21.701500   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:21.816247   21292 main.go:141] libmachine: Using SSH client type: native
I1007 15:41:21.816810   21292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6617c0] 0x664300 <nil>  [] 0s} 127.0.0.1 52128 <nil> <nil>}
I1007 15:41:21.816810   21292 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1007 15:41:24.748486   21292 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-10-07 15:41:21.668205255 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1007 15:41:24.748486   21292 machine.go:96] duration metric: took 5.9900067s to provisionDockerMachine
I1007 15:41:24.748486   21292 client.go:171] duration metric: took 37.0792177s to LocalClient.Create
I1007 15:41:24.748486   21292 start.go:167] duration metric: took 37.0792177s to libmachine.API.Create "minikube"
I1007 15:41:24.749662   21292 start.go:293] postStartSetup for "minikube" (driver="docker")
I1007 15:41:24.749662   21292 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1007 15:41:24.764447   21292 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1007 15:41:24.789319   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:24.905370   21292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52128 SSHKeyPath:C:\Users\mrman\.minikube\machines\minikube\id_rsa Username:docker}
I1007 15:41:25.060472   21292 ssh_runner.go:195] Run: cat /etc/os-release
I1007 15:41:25.072013   21292 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1007 15:41:25.072013   21292 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1007 15:41:25.072013   21292 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1007 15:41:25.072013   21292 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1007 15:41:25.073128   21292 filesync.go:126] Scanning C:\Users\mrman\.minikube\addons for local assets ...
I1007 15:41:25.073694   21292 filesync.go:126] Scanning C:\Users\mrman\.minikube\files for local assets ...
I1007 15:41:25.074370   21292 start.go:296] duration metric: took 324.7079ms for postStartSetup
I1007 15:41:25.101265   21292 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1007 15:41:25.216874   21292 profile.go:143] Saving config to C:\Users\mrman\.minikube\profiles\minikube\config.json ...
I1007 15:41:25.225418   21292 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1007 15:41:25.251517   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:25.357617   21292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52128 SSHKeyPath:C:\Users\mrman\.minikube\machines\minikube\id_rsa Username:docker}
I1007 15:41:25.487708   21292 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1007 15:41:25.502449   21292 start.go:128] duration metric: took 37.8451462s to createHost
I1007 15:41:25.502449   21292 start.go:83] releasing machines lock for "minikube", held for 37.847422s
I1007 15:41:25.530989   21292 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1007 15:41:25.640627   21292 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1007 15:41:25.642818   21292 ssh_runner.go:195] Run: cat /version.json
I1007 15:41:25.668567   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:25.669109   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:25.778866   21292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52128 SSHKeyPath:C:\Users\mrman\.minikube\machines\minikube\id_rsa Username:docker}
I1007 15:41:25.780032   21292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52128 SSHKeyPath:C:\Users\mrman\.minikube\machines\minikube\id_rsa Username:docker}
W1007 15:41:25.912043   21292 start.go:868] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1007 15:41:25.969838   21292 ssh_runner.go:195] Run: systemctl --version
I1007 15:41:25.991733   21292 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1007 15:41:26.019435   21292 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1007 15:41:26.048527   21292 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1007 15:41:26.062643   21292 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1007 15:41:26.154943   21292 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1007 15:41:26.154943   21292 start.go:495] detecting cgroup driver to use...
I1007 15:41:26.155451   21292 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1007 15:41:26.157751   21292 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1007 15:41:26.212736   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1007 15:41:26.248538   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1007 15:41:26.278443   21292 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1007 15:41:26.282930   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1007 15:41:26.315307   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1007 15:41:26.351463   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1007 15:41:26.385807   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1007 15:41:26.421573   21292 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1007 15:41:26.459064   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1007 15:41:26.495637   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1007 15:41:26.535033   21292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1007 15:41:26.584559   21292 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1007 15:41:26.623348   21292 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1007 15:41:26.663634   21292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1007 15:41:26.877664   21292 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1007 15:41:27.088695   21292 start.go:495] detecting cgroup driver to use...
I1007 15:41:27.088695   21292 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1007 15:41:27.106029   21292 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1007 15:41:27.155218   21292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1007 15:41:27.204910   21292 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1007 15:41:27.295286   21292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1007 15:41:27.353684   21292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1007 15:41:27.398148   21292 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1007 15:41:27.468937   21292 ssh_runner.go:195] Run: which cri-dockerd
I1007 15:41:27.498134   21292 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1007 15:41:27.530554   21292 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1007 15:41:27.603290   21292 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1007 15:41:27.790912   21292 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1007 15:41:27.985303   21292 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1007 15:41:27.985303   21292 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1007 15:41:28.049977   21292 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1007 15:41:28.095395   21292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1007 15:41:28.307981   21292 ssh_runner.go:195] Run: sudo systemctl restart docker
W1007 15:41:29.221493   21292 out.go:285] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1007 15:41:29.222075   21292 out.go:285] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1007 15:41:30.483357   21292 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.1753767s)
I1007 15:41:30.496755   21292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1007 15:41:30.544711   21292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1007 15:41:30.588667   21292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1007 15:41:30.636944   21292 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1007 15:41:30.807870   21292 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1007 15:41:31.006909   21292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1007 15:41:31.186111   21292 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1007 15:41:31.266326   21292 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1007 15:41:31.307059   21292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1007 15:41:31.467926   21292 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1007 15:41:31.713821   21292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1007 15:41:31.749227   21292 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1007 15:41:31.753790   21292 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1007 15:41:31.768527   21292 start.go:563] Will wait 60s for crictl version
I1007 15:41:31.774177   21292 ssh_runner.go:195] Run: which crictl
I1007 15:41:31.802383   21292 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1007 15:41:31.909242   21292 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1007 15:41:31.934526   21292 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1007 15:41:32.052090   21292 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1007 15:41:32.145949   21292 out.go:252] 🐳  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1007 15:41:32.175029   21292 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1007 15:41:32.487819   21292 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1007 15:41:32.495775   21292 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1007 15:41:32.512253   21292 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1007 15:41:32.581782   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1007 15:41:32.696383   21292 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1007 15:41:32.696929   21292 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1007 15:41:32.719891   21292 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1007 15:41:32.782896   21292 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1007 15:41:32.782896   21292 docker.go:621] Images already preloaded, skipping extraction
I1007 15:41:32.810919   21292 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1007 15:41:32.859811   21292 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1007 15:41:32.860619   21292 cache_images.go:85] Images are preloaded, skipping loading
I1007 15:41:32.860619   21292 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1007 15:41:32.863897   21292 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1007 15:41:32.887859   21292 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1007 15:41:33.053395   21292 cni.go:84] Creating CNI manager for ""
I1007 15:41:33.053395   21292 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1007 15:41:33.054502   21292 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1007 15:41:33.054502   21292 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1007 15:41:33.054502   21292 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1007 15:41:33.077887   21292 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1007 15:41:33.111998   21292 binaries.go:44] Found k8s binaries, skipping transfer
I1007 15:41:33.126859   21292 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1007 15:41:33.158533   21292 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1007 15:41:33.212740   21292 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1007 15:41:33.259534   21292 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1007 15:41:33.314215   21292 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1007 15:41:33.325448   21292 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1007 15:41:33.367722   21292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1007 15:41:33.546543   21292 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1007 15:41:33.609961   21292 certs.go:68] Setting up C:\Users\mrman\.minikube\profiles\minikube for IP: 192.168.49.2
I1007 15:41:33.609961   21292 certs.go:194] generating shared ca certs ...
I1007 15:41:33.611188   21292 certs.go:226] acquiring lock for ca certs: {Name:mkdafac068aced34aa7633dfa43c5ea5330721ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:33.611790   21292 certs.go:240] generating "minikubeCA" ca cert: C:\Users\mrman\.minikube\ca.key
I1007 15:41:33.760251   21292 crypto.go:156] Writing cert to C:\Users\mrman\.minikube\ca.crt ...
I1007 15:41:33.760251   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\ca.crt: {Name:mkff4eb91130bc48c90be60cce628ab08a7f2991 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:33.762247   21292 crypto.go:164] Writing key to C:\Users\mrman\.minikube\ca.key ...
I1007 15:41:33.762247   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\ca.key: {Name:mkab5b5f6d118a5a951eecb831387562f439e11f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:33.763763   21292 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\mrman\.minikube\proxy-client-ca.key
I1007 15:41:34.121290   21292 crypto.go:156] Writing cert to C:\Users\mrman\.minikube\proxy-client-ca.crt ...
I1007 15:41:34.121290   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\proxy-client-ca.crt: {Name:mk5efa48ae36bd66a083b1969a90653be21182f0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:34.126077   21292 crypto.go:164] Writing key to C:\Users\mrman\.minikube\proxy-client-ca.key ...
I1007 15:41:34.126077   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\proxy-client-ca.key: {Name:mkf1929c6a23d80a8238126aec38f99737abdf5f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:34.128122   21292 certs.go:256] generating profile certs ...
I1007 15:41:34.132879   21292 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\mrman\.minikube\profiles\minikube\client.key
I1007 15:41:34.133893   21292 crypto.go:68] Generating cert C:\Users\mrman\.minikube\profiles\minikube\client.crt with IP's: []
I1007 15:41:34.431526   21292 crypto.go:156] Writing cert to C:\Users\mrman\.minikube\profiles\minikube\client.crt ...
I1007 15:41:34.431526   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\profiles\minikube\client.crt: {Name:mk5f4563b1de12802515005bce07a8663ca1393e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:34.433037   21292 crypto.go:164] Writing key to C:\Users\mrman\.minikube\profiles\minikube\client.key ...
I1007 15:41:34.433037   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\profiles\minikube\client.key: {Name:mkbec35f65747ce5af6086c8320e02fb3bcaa3e2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:34.435578   21292 certs.go:363] generating signed profile cert for "minikube": C:\Users\mrman\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1007 15:41:34.435578   21292 crypto.go:68] Generating cert C:\Users\mrman\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1007 15:41:34.560953   21292 crypto.go:156] Writing cert to C:\Users\mrman\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I1007 15:41:34.560953   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkab10b023539457c0d82de92cd200ff08001137 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:34.563568   21292 crypto.go:164] Writing key to C:\Users\mrman\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I1007 15:41:34.563568   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk0d543178592f83f729a349c46117a1633fb4dc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:34.565586   21292 certs.go:381] copying C:\Users\mrman\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\mrman\.minikube\profiles\minikube\apiserver.crt
I1007 15:41:34.586374   21292 certs.go:385] copying C:\Users\mrman\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\mrman\.minikube\profiles\minikube\apiserver.key
I1007 15:41:34.588851   21292 certs.go:363] generating signed profile cert for "aggregator": C:\Users\mrman\.minikube\profiles\minikube\proxy-client.key
I1007 15:41:34.588851   21292 crypto.go:68] Generating cert C:\Users\mrman\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I1007 15:41:35.233319   21292 crypto.go:156] Writing cert to C:\Users\mrman\.minikube\profiles\minikube\proxy-client.crt ...
I1007 15:41:35.234334   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\profiles\minikube\proxy-client.crt: {Name:mk5eb52ab5b08dde2796e74dbd26cc43bad5230a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:35.237841   21292 crypto.go:164] Writing key to C:\Users\mrman\.minikube\profiles\minikube\proxy-client.key ...
I1007 15:41:35.237841   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.minikube\profiles\minikube\proxy-client.key: {Name:mkae7ae24f40a811adb92e08bf7c758665aba705 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:35.267128   21292 certs.go:484] found cert: C:\Users\mrman\.minikube\certs\ca-key.pem (1675 bytes)
I1007 15:41:35.267674   21292 certs.go:484] found cert: C:\Users\mrman\.minikube\certs\ca.pem (1074 bytes)
I1007 15:41:35.268220   21292 certs.go:484] found cert: C:\Users\mrman\.minikube\certs\cert.pem (1119 bytes)
I1007 15:41:35.268782   21292 certs.go:484] found cert: C:\Users\mrman\.minikube\certs\key.pem (1679 bytes)
I1007 15:41:35.292203   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1007 15:41:35.360708   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1007 15:41:35.441724   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1007 15:41:35.511573   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1007 15:41:35.579873   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1007 15:41:35.644114   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1007 15:41:35.707628   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1007 15:41:35.772935   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1007 15:41:35.839649   21292 ssh_runner.go:362] scp C:\Users\mrman\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1007 15:41:35.909933   21292 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1007 15:41:35.967159   21292 ssh_runner.go:195] Run: openssl version
I1007 15:41:35.997907   21292 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1007 15:41:36.035153   21292 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1007 15:41:36.047182   21292 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct  7 15:41 /usr/share/ca-certificates/minikubeCA.pem
I1007 15:41:36.049999   21292 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1007 15:41:36.079901   21292 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1007 15:41:36.112043   21292 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1007 15:41:36.124847   21292 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1007 15:41:36.125561   21292 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1007 15:41:36.150379   21292 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1007 15:41:36.214468   21292 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1007 15:41:36.256719   21292 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1007 15:41:36.286779   21292 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1007 15:41:36.303157   21292 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1007 15:41:36.331642   21292 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1007 15:41:36.331642   21292 kubeadm.go:157] found existing configuration files:

I1007 15:41:36.345076   21292 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1007 15:41:36.371501   21292 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1007 15:41:36.385569   21292 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1007 15:41:36.424505   21292 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1007 15:41:36.449676   21292 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1007 15:41:36.461996   21292 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1007 15:41:36.500392   21292 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1007 15:41:36.527216   21292 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1007 15:41:36.538804   21292 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1007 15:41:36.576637   21292 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1007 15:41:36.604164   21292 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1007 15:41:36.617055   21292 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1007 15:41:36.641315   21292 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1007 15:41:36.885500   21292 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1007 15:41:37.052576   21292 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1007 15:41:56.035056   21292 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1007 15:41:56.035056   21292 kubeadm.go:310] [preflight] Running pre-flight checks
I1007 15:41:56.035700   21292 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1007 15:41:56.035700   21292 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1007 15:41:56.036121   21292 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1007 15:41:56.036121   21292 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1007 15:41:56.039661   21292 out.go:252]     ▪ Generating certificates and keys ...
I1007 15:41:56.040261   21292 kubeadm.go:310] [certs] Using existing ca certificate authority
I1007 15:41:56.040829   21292 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1007 15:41:56.040829   21292 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1007 15:41:56.040829   21292 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1007 15:41:56.041424   21292 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1007 15:41:56.041424   21292 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1007 15:41:56.041424   21292 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1007 15:41:56.041995   21292 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1007 15:41:56.041995   21292 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1007 15:41:56.041995   21292 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1007 15:41:56.041995   21292 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1007 15:41:56.042697   21292 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1007 15:41:56.042697   21292 kubeadm.go:310] [certs] Generating "sa" key and public key
I1007 15:41:56.042697   21292 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1007 15:41:56.042697   21292 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1007 15:41:56.042697   21292 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1007 15:41:56.043776   21292 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1007 15:41:56.043776   21292 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1007 15:41:56.044313   21292 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1007 15:41:56.044313   21292 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1007 15:41:56.044313   21292 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1007 15:41:56.047779   21292 out.go:252]     ▪ Booting up control plane ...
I1007 15:41:56.049628   21292 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1007 15:41:56.050643   21292 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1007 15:41:56.050643   21292 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1007 15:41:56.051157   21292 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1007 15:41:56.051157   21292 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1007 15:41:56.051157   21292 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1007 15:41:56.051738   21292 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1007 15:41:56.051738   21292 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1007 15:41:56.051738   21292 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1007 15:41:56.051738   21292 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1007 15:41:56.052519   21292 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 2.002797542s
I1007 15:41:56.052519   21292 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1007 15:41:56.052519   21292 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1007 15:41:56.053098   21292 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1007 15:41:56.053098   21292 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1007 15:41:56.053098   21292 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 6.891951405s
I1007 15:41:56.053098   21292 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 7.452819528s
I1007 15:41:56.053635   21292 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 9.502244634s
I1007 15:41:56.053635   21292 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1007 15:41:56.053635   21292 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1007 15:41:56.054225   21292 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1007 15:41:56.055430   21292 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1007 15:41:56.055430   21292 kubeadm.go:310] [bootstrap-token] Using token: lhn9yf.f0mg2mgjeymro6e0
I1007 15:41:56.058935   21292 out.go:252]     ▪ Configuring RBAC rules ...
I1007 15:41:56.059523   21292 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1007 15:41:56.059523   21292 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1007 15:41:56.060101   21292 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1007 15:41:56.060101   21292 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1007 15:41:56.060101   21292 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1007 15:41:56.060101   21292 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1007 15:41:56.060767   21292 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1007 15:41:56.060767   21292 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1007 15:41:56.060767   21292 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1007 15:41:56.060767   21292 kubeadm.go:310] 
I1007 15:41:56.060767   21292 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1007 15:41:56.060767   21292 kubeadm.go:310] 
I1007 15:41:56.061438   21292 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1007 15:41:56.061438   21292 kubeadm.go:310] 
I1007 15:41:56.061438   21292 kubeadm.go:310]   mkdir -p $HOME/.kube
I1007 15:41:56.061438   21292 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1007 15:41:56.061438   21292 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1007 15:41:56.061438   21292 kubeadm.go:310] 
I1007 15:41:56.061438   21292 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1007 15:41:56.061438   21292 kubeadm.go:310] 
I1007 15:41:56.062025   21292 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1007 15:41:56.062025   21292 kubeadm.go:310] 
I1007 15:41:56.062025   21292 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1007 15:41:56.062025   21292 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1007 15:41:56.062025   21292 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1007 15:41:56.062025   21292 kubeadm.go:310] 
I1007 15:41:56.062025   21292 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1007 15:41:56.062610   21292 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1007 15:41:56.062610   21292 kubeadm.go:310] 
I1007 15:41:56.062610   21292 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token lhn9yf.f0mg2mgjeymro6e0 \
I1007 15:41:56.063187   21292 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:4bf53bbcdc458ef10e4964e057a9f3cbc5163769007257cad99130acb17cd876 \
I1007 15:41:56.063187   21292 kubeadm.go:310] 	--control-plane 
I1007 15:41:56.063187   21292 kubeadm.go:310] 
I1007 15:41:56.063187   21292 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1007 15:41:56.063187   21292 kubeadm.go:310] 
I1007 15:41:56.063729   21292 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token lhn9yf.f0mg2mgjeymro6e0 \
I1007 15:41:56.064289   21292 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:4bf53bbcdc458ef10e4964e057a9f3cbc5163769007257cad99130acb17cd876 
I1007 15:41:56.064289   21292 cni.go:84] Creating CNI manager for ""
I1007 15:41:56.064289   21292 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1007 15:41:56.067679   21292 out.go:179] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1007 15:41:56.085508   21292 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1007 15:41:56.244073   21292 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1007 15:41:56.436528   21292 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1007 15:41:56.452289   21292 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_10_07T15_41_56_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1007 15:41:56.456653   21292 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1007 15:41:56.470291   21292 ops.go:34] apiserver oom_adj: -16
I1007 15:41:56.784739   21292 kubeadm.go:1105] duration metric: took 346.465ms to wait for elevateKubeSystemPrivileges
I1007 15:41:56.784739   21292 kubeadm.go:394] duration metric: took 20.6591785s to StartCluster
I1007 15:41:56.785307   21292 settings.go:142] acquiring lock: {Name:mk33b9a5fb2c4559c8343ebcb453f19562fb2f4e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:56.785307   21292 settings.go:150] Updating kubeconfig:  C:\Users\mrman\.kube\config
I1007 15:41:56.789544   21292 lock.go:35] WriteFile acquiring C:\Users\mrman\.kube\config: {Name:mkee33968b7d73291366e2f2621083ef76f64ed0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1007 15:41:56.793412   21292 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1007 15:41:56.793961   21292 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1007 15:41:56.794305   21292 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1007 15:41:56.796317   21292 out.go:179] 🔎  Verifying Kubernetes components...
I1007 15:41:56.795733   21292 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1007 15:41:56.796317   21292 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1007 15:41:56.796317   21292 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1007 15:41:56.797324   21292 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1007 15:41:56.797875   21292 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1007 15:41:56.798461   21292 host.go:66] Checking if "minikube" exists ...
I1007 15:41:56.816646   21292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1007 15:41:56.953517   21292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1007 15:41:56.977371   21292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1007 15:41:57.110458   21292 out.go:179]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1007 15:41:57.112131   21292 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1007 15:41:57.112131   21292 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1007 15:41:57.144528   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:57.154829   21292 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1007 15:41:57.155832   21292 host.go:66] Checking if "minikube" exists ...
I1007 15:41:57.214788   21292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1007 15:41:57.294852   21292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52128 SSHKeyPath:C:\Users\mrman\.minikube\machines\minikube\id_rsa Username:docker}
I1007 15:41:57.377848   21292 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1007 15:41:57.377848   21292 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1007 15:41:57.402863   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1007 15:41:57.464569   21292 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1007 15:41:57.532940   21292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52128 SSHKeyPath:C:\Users\mrman\.minikube\machines\minikube\id_rsa Username:docker}
I1007 15:41:57.666743   21292 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1007 15:41:57.961380   21292 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1007 15:41:58.276578   21292 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1007 15:41:58.963501   21292 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.2967581s)
I1007 15:41:58.963501   21292 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.4989324s)
I1007 15:41:58.963501   21292 start.go:976] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I1007 15:41:58.991206   21292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1007 15:41:59.122021   21292 api_server.go:52] waiting for apiserver process to appear ...
I1007 15:41:59.142497   21292 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1007 15:41:59.537319   21292 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1007 15:41:59.930277   21292 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.6536993s)
I1007 15:41:59.931231   21292 api_server.go:72] duration metric: took 3.1372705s to wait for apiserver process to appear ...
I1007 15:41:59.931231   21292 api_server.go:88] waiting for apiserver healthz status ...
I1007 15:41:59.931231   21292 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.9698508s)
I1007 15:41:59.931762   21292 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52131/healthz ...
I1007 15:41:59.949408   21292 api_server.go:279] https://127.0.0.1:52131/healthz returned 200:
ok
I1007 15:41:59.953439   21292 api_server.go:141] control plane version: v1.34.0
I1007 15:41:59.953439   21292 api_server.go:131] duration metric: took 22.2081ms to wait for apiserver health ...
I1007 15:41:59.953985   21292 system_pods.go:43] waiting for kube-system pods to appear ...
I1007 15:41:59.972751   21292 out.go:179] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1007 15:41:59.974975   21292 addons.go:514] duration metric: took 3.1810145s for enable addons: enabled=[storage-provisioner default-storageclass]
I1007 15:41:59.978293   21292 system_pods.go:59] 5 kube-system pods found
I1007 15:41:59.978919   21292 system_pods.go:61] "etcd-minikube" [f10ac95f-fab8-4e96-83ed-a158a222488f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1007 15:41:59.978919   21292 system_pods.go:61] "kube-apiserver-minikube" [edaee138-4ba2-4e19-905d-58070c76f147] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1007 15:41:59.978919   21292 system_pods.go:61] "kube-controller-manager-minikube" [5841c698-3a3b-48a7-b299-4734fa744632] Running
I1007 15:41:59.978919   21292 system_pods.go:61] "kube-scheduler-minikube" [39c06508-bfe7-4c07-8d8e-b941dd4b3b9b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1007 15:41:59.978919   21292 system_pods.go:61] "storage-provisioner" [20343d63-5a51-45ad-8097-bf90049a3465] Pending / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1007 15:41:59.978919   21292 system_pods.go:74] duration metric: took 24.9347ms to wait for pod list to return data ...
I1007 15:41:59.978919   21292 kubeadm.go:578] duration metric: took 3.1849587s to wait for: map[apiserver:true system_pods:true]
I1007 15:41:59.978919   21292 node_conditions.go:102] verifying NodePressure condition ...
I1007 15:41:59.988550   21292 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1007 15:41:59.988550   21292 node_conditions.go:123] node cpu capacity is 8
I1007 15:41:59.989525   21292 node_conditions.go:105] duration metric: took 10.6054ms to run NodePressure ...
I1007 15:41:59.989525   21292 start.go:241] waiting for startup goroutines ...
I1007 15:41:59.989525   21292 start.go:246] waiting for cluster config update ...
I1007 15:41:59.989525   21292 start.go:255] writing updated cluster config ...
I1007 15:41:59.996492   21292 ssh_runner.go:195] Run: rm -f paused
I1007 15:42:00.320060   21292 start.go:617] kubectl: 1.32.2, cluster: 1.34.0 (minor skew: 2)
I1007 15:42:00.322733   21292 out.go:203] 
W1007 15:42:00.325936   21292 out.go:285] ❗  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.32.2, which may have incompatibilities with Kubernetes 1.34.0.
I1007 15:42:00.339768   21292 out.go:179]     ▪ Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I1007 15:42:00.345602   21292 out.go:179] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 07 15:41:26 minikube dockerd[673]: time="2025-10-07T15:41:26.907029295Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Oct 07 15:41:27 minikube dockerd[673]: time="2025-10-07T15:41:27.234085534Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Oct 07 15:41:27 minikube dockerd[673]: time="2025-10-07T15:41:27.234132779Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Oct 07 15:41:27 minikube dockerd[673]: time="2025-10-07T15:41:27.234106139Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Oct 07 15:41:27 minikube dockerd[673]: time="2025-10-07T15:41:27.234269825Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Oct 07 15:41:28 minikube systemd[1]: Stopping Docker Application Container Engine...
Oct 07 15:41:28 minikube dockerd[673]: time="2025-10-07T15:41:28.333945115Z" level=info msg="Processing signal 'terminated'"
Oct 07 15:41:28 minikube dockerd[673]: time="2025-10-07T15:41:28.336595017Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: grpc: the client connection is closing"
Oct 07 15:41:28 minikube dockerd[673]: time="2025-10-07T15:41:28.337544688Z" level=info msg="Daemon shutdown complete"
Oct 07 15:41:28 minikube dockerd[673]: time="2025-10-07T15:41:28.337631646Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=moby
Oct 07 15:41:28 minikube dockerd[673]: time="2025-10-07T15:41:28.337716315Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: context canceled while waiting for connections to become ready"
Oct 07 15:41:28 minikube dockerd[673]: time="2025-10-07T15:41:28.337736227Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Oct 07 15:41:28 minikube systemd[1]: docker.service: Deactivated successfully.
Oct 07 15:41:28 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 07 15:41:28 minikube systemd[1]: docker.service: Consumed 1.417s CPU time.
Oct 07 15:41:28 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 07 15:41:28 minikube dockerd[1107]: time="2025-10-07T15:41:28.611574894Z" level=info msg="Starting up"
Oct 07 15:41:28 minikube dockerd[1107]: time="2025-10-07T15:41:28.614218474Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Oct 07 15:41:28 minikube dockerd[1107]: time="2025-10-07T15:41:28.614567251Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Oct 07 15:41:28 minikube dockerd[1107]: time="2025-10-07T15:41:28.614625968Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Oct 07 15:41:28 minikube dockerd[1107]: time="2025-10-07T15:41:28.652530845Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Oct 07 15:41:28 minikube dockerd[1107]: time="2025-10-07T15:41:28.672686746Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Oct 07 15:41:28 minikube dockerd[1107]: time="2025-10-07T15:41:28.729039805Z" level=info msg="Loading containers: start."
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.227302996Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count ae1116ea72143176c6ec3275e0854df916a36dfa0da773b0dc4355e539727f6c], retrying...."
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.363539557Z" level=info msg="Loading containers: done."
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.404736073Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.404904661Z" level=info msg="Initializing buildkit"
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.468926368Z" level=info msg="Completed buildkit initialization"
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.476951581Z" level=info msg="Daemon has completed initialization"
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.477256075Z" level=info msg="API listen on /run/docker.sock"
Oct 07 15:41:30 minikube systemd[1]: Started Docker Application Container Engine.
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.477284267Z" level=info msg="API listen on /var/run/docker.sock"
Oct 07 15:41:30 minikube dockerd[1107]: time="2025-10-07T15:41:30.477338260Z" level=info msg="API listen on [::]:2376"
Oct 07 15:41:31 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Start docker client with request timeout 0s"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Loaded network plugin cni"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Setting cgroupDriver cgroupfs"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 07 15:41:31 minikube cri-dockerd[1417]: time="2025-10-07T15:41:31Z" level=info msg="Start cri-dockerd grpc backend"
Oct 07 15:41:31 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 07 15:41:45 minikube cri-dockerd[1417]: time="2025-10-07T15:41:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/02c8ff659da479c1385485b52da8e2ccac80937f7f9818efc2ec2c91adf9a6f7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 07 15:41:45 minikube cri-dockerd[1417]: time="2025-10-07T15:41:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4988e4d91bd0790241d4e101ee72516a655669850017cd3985a58d0b987b2024/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 07 15:41:45 minikube cri-dockerd[1417]: time="2025-10-07T15:41:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/065079e8a045dc6c57d4799e89967e4ea4b5ee0d224978e8c4e1a305a6043a14/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 07 15:41:45 minikube cri-dockerd[1417]: time="2025-10-07T15:41:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2505314336b526e5e4304da235a3b0e63eb817073a660d0801f9f71b33d0225d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 07 15:42:01 minikube cri-dockerd[1417]: time="2025-10-07T15:42:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f99bdd7d8cd3cfee30ec61de3d4d0d20e3db6fcb170e6609631e9fb336a127ca/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 07 15:42:01 minikube cri-dockerd[1417]: time="2025-10-07T15:42:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bc2d8e055d297385ab27670c6068dea2571838faedc4675857367a1c5acb0b01/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 07 15:42:01 minikube cri-dockerd[1417]: time="2025-10-07T15:42:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c4183214f6d99cc9eabcde3d22b3864430c862384ebc9066fe09fb83f65bc9d2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 07 15:42:06 minikube cri-dockerd[1417]: time="2025-10-07T15:42:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Oct 07 15:42:23 minikube dockerd[1107]: time="2025-10-07T15:42:23.654398473Z" level=info msg="ignoring event" container=744c30c045ac8e7386b31a14f17b30a5b168d36baa09449afaf401daa9ab60da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 07 15:45:21 minikube cri-dockerd[1417]: time="2025-10-07T15:45:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ae67b194c38054cbba694b76458541ef1e9aa40a63918e6ffd1370cf2fdd4e44/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 07 15:45:22 minikube cri-dockerd[1417]: time="2025-10-07T15:45:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/44a81dce9ea86493a984a190f822faa8c5d49cc8014147f1272a0546750d768c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 07 15:45:35 minikube cri-dockerd[1417]: time="2025-10-07T15:45:35Z" level=info msg="Pulling image mrman24/kubernetes-demo-api:latest: dd71dde834b5: Downloading [===================================>               ]  28.65MB/40.01MB"
Oct 07 15:45:45 minikube cri-dockerd[1417]: time="2025-10-07T15:45:45Z" level=info msg="Pulling image mrman24/kubernetes-demo-api:latest: 5d5eb99ff7eb: Extracting [==================================================>]   1.83MB/1.83MB"
Oct 07 15:45:45 minikube cri-dockerd[1417]: time="2025-10-07T15:45:45Z" level=info msg="Stop pulling image mrman24/kubernetes-demo-api:latest: Status: Downloaded newer image for mrman24/kubernetes-demo-api:latest"
Oct 07 15:45:48 minikube cri-dockerd[1417]: time="2025-10-07T15:45:48Z" level=info msg="Stop pulling image mrman24/kubernetes-demo-api:latest: Status: Image is up to date for mrman24/kubernetes-demo-api:latest"


==> container status <==
CONTAINER           IMAGE                                                                                                 CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
dd0664c6a7c1c       mrman24/kubernetes-demo-api@sha256:302c66ed258f78866a53e16c4e653f361336ce3a6ac5c750fb6f47150d296530   10 minutes ago      Running             kubernetes-demo-api       0                   44a81dce9ea86       kubernetes-demo-api-66fcf84f5c-t94vp
28d69a44bfe9f       mrman24/kubernetes-demo-api@sha256:302c66ed258f78866a53e16c4e653f361336ce3a6ac5c750fb6f47150d296530   10 minutes ago      Running             kubernetes-demo-api       0                   ae67b194c3805       kubernetes-demo-api-66fcf84f5c-bjxjk
490f5c8b02b91       6e38f40d628db                                                                                         13 minutes ago      Running             storage-provisioner       1                   bc2d8e055d297       storage-provisioner
bb07dca16b9ef       52546a367cc9e                                                                                         14 minutes ago      Running             coredns                   0                   c4183214f6d99       coredns-66bc5c9577-b8qh9
744c30c045ac8       6e38f40d628db                                                                                         14 minutes ago      Exited              storage-provisioner       0                   bc2d8e055d297       storage-provisioner
159f542c20004       df0860106674d                                                                                         14 minutes ago      Running             kube-proxy                0                   f99bdd7d8cd3c       kube-proxy-4gmb2
b937048c7da87       a0af72f2ec6d6                                                                                         14 minutes ago      Running             kube-controller-manager   0                   065079e8a045d       kube-controller-manager-minikube
d2725a9318adb       90550c43ad2bc                                                                                         14 minutes ago      Running             kube-apiserver            0                   2505314336b52       kube-apiserver-minikube
dd6501c0a98d0       46169d968e920                                                                                         14 minutes ago      Running             kube-scheduler            0                   4988e4d91bd07       kube-scheduler-minikube
38eb88a442987       5f1f5298c888d                                                                                         14 minutes ago      Running             etcd                      0                   02c8ff659da47       etcd-minikube


==> coredns [bb07dca16b9e] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:35777 - 63536 "HINFO IN 2314458401360464617.2112274322079929058. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.058902636s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] 10.244.0.3:37202 - 29751 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000906741s
[INFO] 10.244.0.3:37202 - 30321 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.001294683s
[INFO] 10.244.0.3:33915 - 42623 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000484954s
[INFO] 10.244.0.3:33915 - 42033 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000711551s
[INFO] 10.244.0.3:59895 - 57587 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000384613s
[INFO] 10.244.0.3:59895 - 57087 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000567412s
[INFO] 10.244.0.3:32933 - 31100 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 36 0.003877738s
[INFO] 10.244.0.3:32933 - 30640 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.008570236s
[INFO] 10.244.0.4:52936 - 44934 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.001422514s
[INFO] 10.244.0.4:52936 - 44224 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.001741758s
[INFO] 10.244.0.4:46964 - 4977 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000466632s
[INFO] 10.244.0.4:46964 - 4177 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000594794s
[INFO] 10.244.0.4:36374 - 57295 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000285502s
[INFO] 10.244.0.4:36374 - 56845 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000486346s
[INFO] 10.244.0.4:59282 - 26126 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 36 0.000249828s
[INFO] 10.244.0.4:59282 - 25696 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000428269s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_10_07T15_41_56_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 07 Oct 2025 15:41:51 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 07 Oct 2025 15:56:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 07 Oct 2025 15:53:30 +0000   Tue, 07 Oct 2025 15:41:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 07 Oct 2025 15:53:30 +0000   Tue, 07 Oct 2025 15:41:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 07 Oct 2025 15:53:30 +0000   Tue, 07 Oct 2025 15:41:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 07 Oct 2025 15:53:30 +0000   Tue, 07 Oct 2025 15:41:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7049444Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7049444Ki
  pods:               110
System Info:
  Machine ID:                 e77e518fe88049deb974be7817500d60
  System UUID:                e77e518fe88049deb974be7817500d60
  Boot ID:                    e9056518-a12d-4519-8f41-bf58d1c48bc5
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     kubernetes-demo-api-66fcf84f5c-bjxjk    100m (1%)     500m (6%)   128Mi (1%)       512Mi (7%)     10m
  default                     kubernetes-demo-api-66fcf84f5c-t94vp    100m (1%)     500m (6%)   128Mi (1%)       512Mi (7%)     10m
  kube-system                 coredns-66bc5c9577-b8qh9                100m (1%)     0 (0%)      70Mi (1%)        170Mi (2%)     14m
  kube-system                 etcd-minikube                           100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         14m
  kube-system                 kube-apiserver-minikube                 250m (3%)     0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 kube-controller-manager-minikube        200m (2%)     0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 kube-proxy-4gmb2                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 kube-scheduler-minikube                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 storage-provisioner                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (11%)  1 (12%)
  memory             426Mi (6%)  1194Mi (17%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 14m                kube-proxy       
  Normal  Starting                 14m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  14m (x8 over 14m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    14m (x8 over 14m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     14m (x7 over 14m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  14m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 14m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  14m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  14m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    14m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     14m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           14m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Oct 7 11:37] Speculative Return Stack Overflow: IBPB-extending microcode not applied!
[  +0.000001] Speculative Return Stack Overflow: WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options.
[  +0.072451] PCI: Fatal: No config space access function found
[  +0.056803] PCI: System does not support PCI
[  +0.661350] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +6.413791] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#191 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.015748] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#255 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.130641] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#319 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.019540] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#383 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.021488] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#447 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.019443] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#511 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.017919] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#575 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.017914] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#7 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +2.831986] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2119: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.031836] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Atlantic/Reykjavik not found. Is the tzdata package installed?
[  +0.662685] pulseaudio[263]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.233859] WSL (213) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.236832] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#6 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.026433] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#127 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.026132] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.011627] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002077] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005839] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001989] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Oct 7 11:38] netlink: 'init': attribute type 4 has an invalid length.
[Oct 7 14:12] WSL (213) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [38eb88a44298] <==
{"level":"warn","ts":"2025-10-07T15:41:49.358944Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47360","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.428162Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47374","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.440460Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47400","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.456172Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47426","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.465427Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47454","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.542158Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47492","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.552593Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47496","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.562548Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47502","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.640316Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47520","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.652259Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47534","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.665117Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47550","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-07T15:41:49.866257Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:47570","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-10-07T15:41:51.528062Z","caller":"traceutil/trace.go:172","msg":"trace[152025843] transaction","detail":"{read_only:false; response_revision:3; number_of_response:1; }","duration":"184.61865ms","start":"2025-10-07T15:41:51.343414Z","end":"2025-10-07T15:41:51.528033Z","steps":["trace[152025843] 'process raft request'  (duration: 183.135661ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:51.572180Z","caller":"traceutil/trace.go:172","msg":"trace[845344905] transaction","detail":"{read_only:false; response_revision:5; number_of_response:1; }","duration":"134.893324ms","start":"2025-10-07T15:41:51.437264Z","end":"2025-10-07T15:41:51.572158Z","steps":["trace[845344905] 'process raft request'  (duration: 103.599175ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:51.572633Z","caller":"traceutil/trace.go:172","msg":"trace[1937268891] transaction","detail":"{read_only:false; response_revision:6; number_of_response:1; }","duration":"135.042069ms","start":"2025-10-07T15:41:51.437580Z","end":"2025-10-07T15:41:51.572622Z","steps":["trace[1937268891] 'process raft request'  (duration: 114.718396ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:51.629766Z","caller":"traceutil/trace.go:172","msg":"trace[1076543840] transaction","detail":"{read_only:false; response_revision:7; number_of_response:1; }","duration":"191.452911ms","start":"2025-10-07T15:41:51.438219Z","end":"2025-10-07T15:41:51.629672Z","steps":["trace[1076543840] 'process raft request'  (duration: 116.061615ms)","trace[1076543840] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/prioritylevelconfigurations/system; req_size:685; } (duration: 73.236032ms)"],"step_count":2}
{"level":"info","ts":"2025-10-07T15:41:51.630341Z","caller":"traceutil/trace.go:172","msg":"trace[505070388] transaction","detail":"{read_only:false; response_revision:10; number_of_response:1; }","duration":"185.383373ms","start":"2025-10-07T15:41:51.444910Z","end":"2025-10-07T15:41:51.630294Z","steps":["trace[505070388] 'process raft request'  (duration: 184.90968ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:51.630419Z","caller":"traceutil/trace.go:172","msg":"trace[1364204330] transaction","detail":"{read_only:false; response_revision:9; number_of_response:1; }","duration":"191.107304ms","start":"2025-10-07T15:41:51.439299Z","end":"2025-10-07T15:41:51.630407Z","steps":["trace[1364204330] 'process raft request'  (duration: 189.840585ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:51.629935Z","caller":"traceutil/trace.go:172","msg":"trace[1685303920] transaction","detail":"{read_only:false; response_revision:8; number_of_response:1; }","duration":"191.098949ms","start":"2025-10-07T15:41:51.438800Z","end":"2025-10-07T15:41:51.629899Z","steps":["trace[1685303920] 'process raft request'  (duration: 190.183961ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:51.631071Z","caller":"traceutil/trace.go:172","msg":"trace[1599899925] transaction","detail":"{read_only:false; response_revision:12; number_of_response:1; }","duration":"185.629762ms","start":"2025-10-07T15:41:51.445397Z","end":"2025-10-07T15:41:51.631027Z","steps":["trace[1599899925] 'process raft request'  (duration: 184.783899ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:51.631162Z","caller":"traceutil/trace.go:172","msg":"trace[1239540458] transaction","detail":"{read_only:false; response_revision:11; number_of_response:1; }","duration":"186.105551ms","start":"2025-10-07T15:41:51.445044Z","end":"2025-10-07T15:41:51.631149Z","steps":["trace[1239540458] 'process raft request'  (duration: 185.045285ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:41:57.255127Z","caller":"traceutil/trace.go:172","msg":"trace[1600627565] transaction","detail":"{read_only:false; response_revision:315; number_of_response:1; }","duration":"124.67167ms","start":"2025-10-07T15:41:57.130432Z","end":"2025-10-07T15:41:57.255103Z","steps":["trace[1600627565] 'process raft request'  (duration: 97.30765ms)","trace[1600627565] 'compare'  (duration: 26.937244ms)"],"step_count":2}
{"level":"info","ts":"2025-10-07T15:41:57.846544Z","caller":"traceutil/trace.go:172","msg":"trace[805553895] transaction","detail":"{read_only:false; response_revision:322; number_of_response:1; }","duration":"107.17397ms","start":"2025-10-07T15:41:57.739343Z","end":"2025-10-07T15:41:57.846517Z","steps":["trace[805553895] 'process raft request'  (duration: 86.687417ms)","trace[805553895] 'compare'  (duration: 20.165273ms)"],"step_count":2}
{"level":"info","ts":"2025-10-07T15:42:43.047820Z","caller":"traceutil/trace.go:172","msg":"trace[1066896022] transaction","detail":"{read_only:false; response_revision:449; number_of_response:1; }","duration":"105.649773ms","start":"2025-10-07T15:42:42.942136Z","end":"2025-10-07T15:42:43.047785Z","steps":["trace[1066896022] 'process raft request'  (duration: 103.420865ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:43:03.286393Z","caller":"traceutil/trace.go:172","msg":"trace[1177556938] transaction","detail":"{read_only:false; response_revision:464; number_of_response:1; }","duration":"119.578699ms","start":"2025-10-07T15:43:03.166789Z","end":"2025-10-07T15:43:03.286368Z","steps":["trace[1177556938] 'process raft request'  (duration: 119.183323ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:43:29.706418Z","caller":"traceutil/trace.go:172","msg":"trace[400009304] transaction","detail":"{read_only:false; response_revision:486; number_of_response:1; }","duration":"121.415746ms","start":"2025-10-07T15:43:29.584988Z","end":"2025-10-07T15:43:29.706403Z","steps":["trace[400009304] 'process raft request'  (duration: 121.231845ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:43:44.598792Z","caller":"traceutil/trace.go:172","msg":"trace[687568445] transaction","detail":"{read_only:false; response_revision:498; number_of_response:1; }","duration":"321.248339ms","start":"2025-10-07T15:43:44.277513Z","end":"2025-10-07T15:43:44.598761Z","steps":["trace[687568445] 'process raft request'  (duration: 304.886726ms)","trace[687568445] 'compare'  (duration: 14.76187ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-07T15:43:44.599629Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-07T15:43:44.277485Z","time spent":"321.454412ms","remote":"127.0.0.1:46892","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:490 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2025-10-07T15:43:55.159794Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"381.791051ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:43:55.159914Z","caller":"traceutil/trace.go:172","msg":"trace[173513786] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:505; }","duration":"381.926506ms","start":"2025-10-07T15:43:54.777966Z","end":"2025-10-07T15:43:55.159893Z","steps":["trace[173513786] 'range keys from in-memory index tree'  (duration: 381.734295ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:43:55.160080Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"485.702429ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:43:55.160137Z","caller":"traceutil/trace.go:172","msg":"trace[449581870] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:505; }","duration":"485.766987ms","start":"2025-10-07T15:43:54.674356Z","end":"2025-10-07T15:43:55.160123Z","steps":["trace[449581870] 'range keys from in-memory index tree'  (duration: 485.647563ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:43:55.160182Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-07T15:43:54.674337Z","time spent":"485.833184ms","remote":"127.0.0.1:46368","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-10-07T15:43:55.172999Z","caller":"traceutil/trace.go:172","msg":"trace[1543734838] transaction","detail":"{read_only:false; response_revision:506; number_of_response:1; }","duration":"180.590435ms","start":"2025-10-07T15:43:54.992373Z","end":"2025-10-07T15:43:55.172963Z","steps":["trace[1543734838] 'process raft request'  (duration: 180.291975ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:44:02.273848Z","caller":"traceutil/trace.go:172","msg":"trace[2022262794] transaction","detail":"{read_only:false; response_revision:511; number_of_response:1; }","duration":"249.034136ms","start":"2025-10-07T15:44:02.024787Z","end":"2025-10-07T15:44:02.273821Z","steps":["trace[2022262794] 'process raft request'  (duration: 248.703269ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:44:14.571767Z","caller":"traceutil/trace.go:172","msg":"trace[2115455536] transaction","detail":"{read_only:false; response_revision:521; number_of_response:1; }","duration":"184.917682ms","start":"2025-10-07T15:44:14.386832Z","end":"2025-10-07T15:44:14.571750Z","steps":["trace[2115455536] 'process raft request'  (duration: 184.682455ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:44:26.924714Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"149.698708ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:44:26.924918Z","caller":"traceutil/trace.go:172","msg":"trace[730912796] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:531; }","duration":"149.911441ms","start":"2025-10-07T15:44:26.774986Z","end":"2025-10-07T15:44:26.924897Z","steps":["trace[730912796] 'range keys from in-memory index tree'  (duration: 149.657094ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:44:40.928201Z","caller":"traceutil/trace.go:172","msg":"trace[1231000610] transaction","detail":"{read_only:false; response_revision:542; number_of_response:1; }","duration":"106.437987ms","start":"2025-10-07T15:44:40.821734Z","end":"2025-10-07T15:44:40.928172Z","steps":["trace[1231000610] 'process raft request'  (duration: 106.211189ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:45:09.798708Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"131.387254ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:45:09.798806Z","caller":"traceutil/trace.go:172","msg":"trace[772948277] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:565; }","duration":"131.498566ms","start":"2025-10-07T15:45:09.667294Z","end":"2025-10-07T15:45:09.798793Z","steps":["trace[772948277] 'range keys from in-memory index tree'  (duration: 131.242486ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:45:09.798864Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"146.816661ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:45:09.798906Z","caller":"traceutil/trace.go:172","msg":"trace[656157968] range","detail":"{range_begin:/registry/namespaces; range_end:; response_count:0; response_revision:565; }","duration":"146.861566ms","start":"2025-10-07T15:45:09.652032Z","end":"2025-10-07T15:45:09.798893Z","steps":["trace[656157968] 'range keys from in-memory index tree'  (duration: 146.736872ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:46:02.187052Z","caller":"traceutil/trace.go:172","msg":"trace[1840271034] transaction","detail":"{read_only:false; response_revision:646; number_of_response:1; }","duration":"199.832493ms","start":"2025-10-07T15:46:01.987202Z","end":"2025-10-07T15:46:02.187035Z","steps":["trace[1840271034] 'process raft request'  (duration: 199.640682ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:47:47.837011Z","caller":"traceutil/trace.go:172","msg":"trace[1651015204] transaction","detail":"{read_only:false; response_revision:730; number_of_response:1; }","duration":"274.858811ms","start":"2025-10-07T15:47:47.562129Z","end":"2025-10-07T15:47:47.836988Z","steps":["trace[1651015204] 'process raft request'  (duration: 274.697663ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:48:06.112792Z","caller":"traceutil/trace.go:172","msg":"trace[1765088112] transaction","detail":"{read_only:false; response_revision:745; number_of_response:1; }","duration":"113.20669ms","start":"2025-10-07T15:48:05.999568Z","end":"2025-10-07T15:48:06.112775Z","steps":["trace[1765088112] 'process raft request'  (duration: 113.058933ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:48:06.457331Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"208.749282ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:48:06.457417Z","caller":"traceutil/trace.go:172","msg":"trace[1865184655] range","detail":"{range_begin:/registry/storageclasses; range_end:; response_count:0; response_revision:745; }","duration":"208.843873ms","start":"2025-10-07T15:48:06.248556Z","end":"2025-10-07T15:48:06.457400Z","steps":["trace[1865184655] 'range keys from in-memory index tree'  (duration: 208.680283ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:48:38.857732Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"102.304785ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:48:38.857851Z","caller":"traceutil/trace.go:172","msg":"trace[1990889572] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:770; }","duration":"102.427278ms","start":"2025-10-07T15:48:38.755404Z","end":"2025-10-07T15:48:38.857832Z","steps":["trace[1990889572] 'range keys from in-memory index tree'  (duration: 102.253708ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-07T15:49:29.300108Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"106.5659ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-10-07T15:49:29.300248Z","caller":"traceutil/trace.go:172","msg":"trace[1112574590] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:809; }","duration":"106.714007ms","start":"2025-10-07T15:49:29.193515Z","end":"2025-10-07T15:49:29.300229Z","steps":["trace[1112574590] 'agreement among raft nodes before linearized reading'  (duration: 80.130292ms)","trace[1112574590] 'range keys from in-memory index tree'  (duration: 26.348679ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-07T15:49:40.872071Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"121.23738ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-07T15:49:40.872382Z","caller":"traceutil/trace.go:172","msg":"trace[1561320514] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:819; }","duration":"121.557463ms","start":"2025-10-07T15:49:40.750793Z","end":"2025-10-07T15:49:40.872351Z","steps":["trace[1561320514] 'range keys from in-memory index tree'  (duration: 121.187445ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:51:47.875176Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":683}
{"level":"info","ts":"2025-10-07T15:51:47.911317Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":683,"took":"35.775923ms","hash":2416749080,"current-db-size-bytes":2150400,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":2150400,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-10-07T15:51:47.911433Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2416749080,"revision":683,"compact-revision":-1}
{"level":"info","ts":"2025-10-07T15:51:48.435466Z","caller":"traceutil/trace.go:172","msg":"trace[1676443699] transaction","detail":"{read_only:false; response_revision:921; number_of_response:1; }","duration":"127.602774ms","start":"2025-10-07T15:51:48.307847Z","end":"2025-10-07T15:51:48.435450Z","steps":["trace[1676443699] 'process raft request'  (duration: 127.435615ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:54:26.485414Z","caller":"traceutil/trace.go:172","msg":"trace[1104084168] transaction","detail":"{read_only:false; response_revision:1047; number_of_response:1; }","duration":"146.006196ms","start":"2025-10-07T15:54:26.339381Z","end":"2025-10-07T15:54:26.485387Z","steps":["trace[1104084168] 'process raft request'  (duration: 145.74819ms)"],"step_count":1}
{"level":"info","ts":"2025-10-07T15:55:09.797194Z","caller":"traceutil/trace.go:172","msg":"trace[1324272592] transaction","detail":"{read_only:false; response_revision:1081; number_of_response:1; }","duration":"104.34858ms","start":"2025-10-07T15:55:09.692821Z","end":"2025-10-07T15:55:09.797170Z","steps":["trace[1324272592] 'process raft request'  (duration: 104.143337ms)"],"step_count":1}


==> kernel <==
 15:56:18 up  4:19,  0 users,  load average: 0.52, 0.46, 0.39
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [d2725a9318ad] <==
I1007 15:41:51.244044       1 aggregator.go:171] initial CRD sync complete...
I1007 15:41:51.244115       1 autoregister_controller.go:144] Starting autoregister controller
I1007 15:41:51.244191       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1007 15:41:51.244257       1 cache.go:39] Caches are synced for autoregister controller
I1007 15:41:51.325998       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1007 15:41:51.329462       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1007 15:41:51.336014       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1007 15:41:51.352558       1 controller.go:667] quota admission added evaluator for: namespaces
I1007 15:41:51.427106       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1007 15:41:51.427303       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1007 15:41:51.428336       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1007 15:41:51.429744       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1007 15:41:51.448436       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1007 15:41:51.526664       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
E1007 15:41:51.529940       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E1007 15:41:51.530884       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I1007 15:41:51.634041       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1007 15:41:51.640907       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1007 15:41:51.748690       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1007 15:41:52.087501       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1007 15:41:52.126982       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1007 15:41:52.127054       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1007 15:41:53.842878       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1007 15:41:53.978134       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1007 15:41:54.153201       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1007 15:41:54.174910       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1007 15:41:54.176972       1 controller.go:667] quota admission added evaluator for: endpoints
I1007 15:41:54.189597       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1007 15:41:54.212671       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1007 15:41:55.613035       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1007 15:41:55.647980       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1007 15:41:55.729594       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1007 15:41:59.788075       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1007 15:41:59.805037       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1007 15:42:00.114996       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1007 15:42:00.360955       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1007 15:43:13.178160       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:43:13.637394       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:44:17.646415       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:44:24.812189       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:45:21.103671       1 alloc.go:328] "allocated clusterIPs" service="default/devops-kubernetes-api-service" clusterIPs={"IPv4":"10.109.228.210"}
I1007 15:45:32.248216       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:45:40.375874       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:46:41.346878       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:47:00.092465       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:47:55.486914       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:48:23.862346       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:49:17.508948       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:49:25.601692       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:50:45.387844       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:50:47.024562       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:51:51.178555       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1007 15:51:55.440964       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:51:56.075425       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:52:58.439546       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:53:08.886545       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:54:23.385342       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:54:24.642095       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:55:27.667125       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1007 15:55:33.237817       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [b937048c7da8] <==
I1007 15:41:58.870709       1 shared_informer.go:349] "Waiting for caches to sync" controller="daemon sets"
I1007 15:41:59.143639       1 controllermanager.go:781] "Started controller" controller="namespace-controller"
I1007 15:41:59.143926       1 namespace_controller.go:202] "Starting namespace controller" logger="namespace-controller"
I1007 15:41:59.144021       1 shared_informer.go:349] "Waiting for caches to sync" controller="namespace"
I1007 15:41:59.158947       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1007 15:41:59.250526       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1007 15:41:59.257230       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1007 15:41:59.327586       1 shared_informer.go:356] "Caches are synced" controller="node"
I1007 15:41:59.328376       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1007 15:41:59.329186       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1007 15:41:59.336806       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1007 15:41:59.337689       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1007 15:41:59.340360       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1007 15:41:59.340448       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1007 15:41:59.340525       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1007 15:41:59.348095       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1007 15:41:59.426059       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1007 15:41:59.426410       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1007 15:41:59.426492       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1007 15:41:59.426769       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1007 15:41:59.426997       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1007 15:41:59.427539       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1007 15:41:59.427712       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1007 15:41:59.435801       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1007 15:41:59.436932       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1007 15:41:59.441027       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1007 15:41:59.441685       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1007 15:41:59.444402       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1007 15:41:59.446667       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1007 15:41:59.445793       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1007 15:41:59.444732       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1007 15:41:59.448915       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1007 15:41:59.451214       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1007 15:41:59.525999       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1007 15:41:59.531561       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1007 15:41:59.531778       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1007 15:41:59.526231       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1007 15:41:59.526297       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1007 15:41:59.526364       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1007 15:41:59.526260       1 shared_informer.go:356] "Caches are synced" controller="job"
I1007 15:41:59.532419       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1007 15:41:59.526422       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1007 15:41:59.526193       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1007 15:41:59.526442       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1007 15:41:59.526588       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1007 15:41:59.526577       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1007 15:41:59.532675       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1007 15:41:59.527067       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1007 15:41:59.528621       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1007 15:41:59.526506       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1007 15:41:59.533019       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1007 15:41:59.526388       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1007 15:41:59.527923       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1007 15:41:59.533819       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1007 15:41:59.535646       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1007 15:41:59.536312       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1007 15:41:59.540608       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1007 15:41:59.626593       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1007 15:41:59.626633       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1007 15:41:59.626657       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [159f542c2000] <==
I1007 15:42:02.653221       1 server_linux.go:53] "Using iptables proxy"
I1007 15:42:02.931344       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1007 15:42:03.032654       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1007 15:42:03.032726       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1007 15:42:03.032843       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1007 15:42:03.087806       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1007 15:42:03.087904       1 server_linux.go:132] "Using iptables Proxier"
I1007 15:42:03.100968       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1007 15:42:03.116292       1 server.go:527] "Version info" version="v1.34.0"
I1007 15:42:03.116374       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1007 15:42:03.121057       1 config.go:200] "Starting service config controller"
I1007 15:42:03.121107       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1007 15:42:03.121171       1 config.go:403] "Starting serviceCIDR config controller"
I1007 15:42:03.121180       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1007 15:42:03.121196       1 config.go:106] "Starting endpoint slice config controller"
I1007 15:42:03.121219       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1007 15:42:03.122563       1 config.go:309] "Starting node config controller"
I1007 15:42:03.122646       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1007 15:42:03.122662       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1007 15:42:03.221383       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1007 15:42:03.221451       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1007 15:42:03.221427       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [dd6501c0a98d] <==
I1007 15:41:48.467455       1 serving.go:386] Generated self-signed cert in-memory
W1007 15:41:51.330884       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1007 15:41:51.331083       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1007 15:41:51.331272       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1007 15:41:51.331538       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1007 15:41:51.538525       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1007 15:41:51.538578       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1007 15:41:51.544028       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1007 15:41:51.544131       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1007 15:41:51.544545       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1007 15:41:51.544589       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1007 15:41:51.552134       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1007 15:41:51.552134       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1007 15:41:51.552264       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1007 15:41:51.552459       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1007 15:41:51.552991       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1007 15:41:51.553230       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1007 15:41:51.553321       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1007 15:41:51.553349       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1007 15:41:51.553436       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1007 15:41:51.553564       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1007 15:41:51.553740       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1007 15:41:51.553913       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1007 15:41:51.554082       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1007 15:41:51.554166       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1007 15:41:51.556435       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1007 15:41:51.556780       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1007 15:41:51.630847       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1007 15:41:51.631089       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1007 15:41:51.631720       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1007 15:41:52.372759       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1007 15:41:52.379906       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1007 15:41:52.463627       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1007 15:41:52.510763       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1007 15:41:52.700130       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1007 15:41:52.741698       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1007 15:41:52.768332       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1007 15:41:52.810079       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1007 15:41:52.911241       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1007 15:41:52.921636       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1007 15:41:52.953367       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1007 15:41:52.954609       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1007 15:41:52.983568       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1007 15:41:52.990648       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1007 15:41:53.011882       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1007 15:41:53.013089       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1007 15:41:53.124241       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1007 15:41:53.126290       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1007 15:41:53.195404       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
I1007 15:41:56.144570       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232052    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232097    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232137    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232232    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232283    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232337    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232373    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232406    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232440    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/dc6cf0a7bcb54d1f95cecc4d7b6b7d67-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"dc6cf0a7bcb54d1f95cecc4d7b6b7d67\") " pod="kube-system/kube-scheduler-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232482    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/e3a36fac0ae701bc11fad0a6716eec2c-etcd-certs\") pod \"etcd-minikube\" (UID: \"e3a36fac0ae701bc11fad0a6716eec2c\") " pod="kube-system/etcd-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232512    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.232546    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.235392    2348 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.250577    2348 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Oct 07 15:41:56 minikube kubelet[2348]: E1007 15:41:56.326412    2348 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.327956    2348 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.345473    2348 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.361554    2348 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.361709    2348 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Oct 07 15:41:56 minikube kubelet[2348]: E1007 15:41:56.367244    2348 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.783844    2348 apiserver.go:52] "Watching apiserver"
Oct 07 15:41:56 minikube kubelet[2348]: I1007 15:41:56.829584    2348 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Oct 07 15:41:57 minikube kubelet[2348]: I1007 15:41:57.027266    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=2.027238137 podStartE2EDuration="2.027238137s" podCreationTimestamp="2025-10-07 15:41:55 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-07 15:41:56.949861589 +0000 UTC m=+1.386099110" watchObservedRunningTime="2025-10-07 15:41:57.027238137 +0000 UTC m=+1.463475658"
Oct 07 15:41:57 minikube kubelet[2348]: I1007 15:41:57.258791    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.258757448 podStartE2EDuration="1.258757448s" podCreationTimestamp="2025-10-07 15:41:56 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-07 15:41:57.258202119 +0000 UTC m=+1.694439640" watchObservedRunningTime="2025-10-07 15:41:57.258757448 +0000 UTC m=+1.694994949"
Oct 07 15:41:57 minikube kubelet[2348]: I1007 15:41:57.259078    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.259062231 podStartE2EDuration="1.259062231s" podCreationTimestamp="2025-10-07 15:41:56 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-07 15:41:57.06032303 +0000 UTC m=+1.496560561" watchObservedRunningTime="2025-10-07 15:41:57.259062231 +0000 UTC m=+1.695299752"
Oct 07 15:41:57 minikube kubelet[2348]: I1007 15:41:57.359871    2348 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:57 minikube kubelet[2348]: I1007 15:41:57.360204    2348 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Oct 07 15:41:57 minikube kubelet[2348]: I1007 15:41:57.360705    2348 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Oct 07 15:41:57 minikube kubelet[2348]: I1007 15:41:57.361811    2348 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Oct 07 15:41:57 minikube kubelet[2348]: E1007 15:41:57.455522    2348 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Oct 07 15:41:57 minikube kubelet[2348]: E1007 15:41:57.461837    2348 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Oct 07 15:41:57 minikube kubelet[2348]: E1007 15:41:57.462196    2348 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Oct 07 15:41:57 minikube kubelet[2348]: E1007 15:41:57.462133    2348 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.044837    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/20343d63-5a51-45ad-8097-bf90049a3465-tmp\") pod \"storage-provisioner\" (UID: \"20343d63-5a51-45ad-8097-bf90049a3465\") " pod="kube-system/storage-provisioner"
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.044992    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qthw6\" (UniqueName: \"kubernetes.io/projected/20343d63-5a51-45ad-8097-bf90049a3465-kube-api-access-qthw6\") pod \"storage-provisioner\" (UID: \"20343d63-5a51-45ad-8097-bf90049a3465\") " pod="kube-system/storage-provisioner"
Oct 07 15:42:00 minikube kubelet[2348]: E1007 15:42:00.158230    2348 projected.go:291] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Oct 07 15:42:00 minikube kubelet[2348]: E1007 15:42:00.158549    2348 projected.go:196] Error preparing data for projected volume kube-api-access-qthw6 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Oct 07 15:42:00 minikube kubelet[2348]: E1007 15:42:00.158786    2348 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/20343d63-5a51-45ad-8097-bf90049a3465-kube-api-access-qthw6 podName:20343d63-5a51-45ad-8097-bf90049a3465 nodeName:}" failed. No retries permitted until 2025-10-07 15:42:00.658715092 +0000 UTC m=+5.094952603 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-qthw6" (UniqueName: "kubernetes.io/projected/20343d63-5a51-45ad-8097-bf90049a3465-kube-api-access-qthw6") pod "storage-provisioner" (UID: "20343d63-5a51-45ad-8097-bf90049a3465") : configmap "kube-root-ca.crt" not found
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.449128    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/29a77f77-270e-4da9-ba44-266c905c9d54-xtables-lock\") pod \"kube-proxy-4gmb2\" (UID: \"29a77f77-270e-4da9-ba44-266c905c9d54\") " pod="kube-system/kube-proxy-4gmb2"
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.449244    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/29a77f77-270e-4da9-ba44-266c905c9d54-kube-proxy\") pod \"kube-proxy-4gmb2\" (UID: \"29a77f77-270e-4da9-ba44-266c905c9d54\") " pod="kube-system/kube-proxy-4gmb2"
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.449282    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6zwj6\" (UniqueName: \"kubernetes.io/projected/29a77f77-270e-4da9-ba44-266c905c9d54-kube-api-access-6zwj6\") pod \"kube-proxy-4gmb2\" (UID: \"29a77f77-270e-4da9-ba44-266c905c9d54\") " pod="kube-system/kube-proxy-4gmb2"
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.449334    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/29a77f77-270e-4da9-ba44-266c905c9d54-lib-modules\") pod \"kube-proxy-4gmb2\" (UID: \"29a77f77-270e-4da9-ba44-266c905c9d54\") " pod="kube-system/kube-proxy-4gmb2"
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.549833    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/2d5ec8f3-4c64-4cf0-b541-42c23e403eb2-config-volume\") pod \"coredns-66bc5c9577-b8qh9\" (UID: \"2d5ec8f3-4c64-4cf0-b541-42c23e403eb2\") " pod="kube-system/coredns-66bc5c9577-b8qh9"
Oct 07 15:42:00 minikube kubelet[2348]: I1007 15:42:00.552986    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-254rk\" (UniqueName: \"kubernetes.io/projected/2d5ec8f3-4c64-4cf0-b541-42c23e403eb2-kube-api-access-254rk\") pod \"coredns-66bc5c9577-b8qh9\" (UID: \"2d5ec8f3-4c64-4cf0-b541-42c23e403eb2\") " pod="kube-system/coredns-66bc5c9577-b8qh9"
Oct 07 15:42:01 minikube kubelet[2348]: I1007 15:42:01.736144    2348 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bc2d8e055d297385ab27670c6068dea2571838faedc4675857367a1c5acb0b01"
Oct 07 15:42:01 minikube kubelet[2348]: I1007 15:42:01.768043    2348 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c4183214f6d99cc9eabcde3d22b3864430c862384ebc9066fe09fb83f65bc9d2"
Oct 07 15:42:02 minikube kubelet[2348]: I1007 15:42:02.938575    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=3.938549573 podStartE2EDuration="3.938549573s" podCreationTimestamp="2025-10-07 15:41:59 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-07 15:42:02.849171895 +0000 UTC m=+7.285409416" watchObservedRunningTime="2025-10-07 15:42:02.938549573 +0000 UTC m=+7.374787104"
Oct 07 15:42:03 minikube kubelet[2348]: I1007 15:42:03.422651    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-4gmb2" podStartSLOduration=3.4226279379999998 podStartE2EDuration="3.422627938s" podCreationTimestamp="2025-10-07 15:42:00 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-07 15:42:02.968094452 +0000 UTC m=+7.404332103" watchObservedRunningTime="2025-10-07 15:42:03.422627938 +0000 UTC m=+7.858865449"
Oct 07 15:42:04 minikube kubelet[2348]: I1007 15:42:04.973533    2348 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 07 15:42:06 minikube kubelet[2348]: I1007 15:42:06.689424    2348 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 07 15:42:06 minikube kubelet[2348]: I1007 15:42:06.690974    2348 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 07 15:42:06 minikube kubelet[2348]: I1007 15:42:06.806634    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-66bc5c9577-b8qh9" podStartSLOduration=6.806616496 podStartE2EDuration="6.806616496s" podCreationTimestamp="2025-10-07 15:42:00 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-07 15:42:04.015938937 +0000 UTC m=+8.452176458" watchObservedRunningTime="2025-10-07 15:42:06.806616496 +0000 UTC m=+11.242853997"
Oct 07 15:42:10 minikube kubelet[2348]: I1007 15:42:10.466309    2348 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 07 15:42:24 minikube kubelet[2348]: I1007 15:42:24.146840    2348 scope.go:117] "RemoveContainer" containerID="744c30c045ac8e7386b31a14f17b30a5b168d36baa09449afaf401daa9ab60da"
Oct 07 15:45:21 minikube kubelet[2348]: I1007 15:45:21.233823    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k88mw\" (UniqueName: \"kubernetes.io/projected/aa4b52a1-4ca2-4826-b162-ed7dafad1470-kube-api-access-k88mw\") pod \"kubernetes-demo-api-66fcf84f5c-t94vp\" (UID: \"aa4b52a1-4ca2-4826-b162-ed7dafad1470\") " pod="default/kubernetes-demo-api-66fcf84f5c-t94vp"
Oct 07 15:45:21 minikube kubelet[2348]: I1007 15:45:21.233935    2348 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qvcl7\" (UniqueName: \"kubernetes.io/projected/231e94c7-5773-46ba-bd65-058b026573f8-kube-api-access-qvcl7\") pod \"kubernetes-demo-api-66fcf84f5c-bjxjk\" (UID: \"231e94c7-5773-46ba-bd65-058b026573f8\") " pod="default/kubernetes-demo-api-66fcf84f5c-bjxjk"
Oct 07 15:45:22 minikube kubelet[2348]: I1007 15:45:22.001093    2348 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ae67b194c38054cbba694b76458541ef1e9aa40a63918e6ffd1370cf2fdd4e44"
Oct 07 15:45:22 minikube kubelet[2348]: I1007 15:45:22.083649    2348 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="44a81dce9ea86493a984a190f822faa8c5d49cc8014147f1272a0546750d768c"
Oct 07 15:45:47 minikube kubelet[2348]: I1007 15:45:47.553200    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/kubernetes-demo-api-66fcf84f5c-bjxjk" podStartSLOduration=2.9399797420000002 podStartE2EDuration="26.553135051s" podCreationTimestamp="2025-10-07 15:45:21 +0000 UTC" firstStartedPulling="2025-10-07 15:45:22.165760879 +0000 UTC m=+206.620548903" lastFinishedPulling="2025-10-07 15:45:45.778916178 +0000 UTC m=+230.233704212" observedRunningTime="2025-10-07 15:45:47.553122982 +0000 UTC m=+232.007911036" watchObservedRunningTime="2025-10-07 15:45:47.553135051 +0000 UTC m=+232.007923095"
Oct 07 15:45:58 minikube kubelet[2348]: I1007 15:45:58.611374    2348 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/kubernetes-demo-api-66fcf84f5c-t94vp" podStartSLOduration=11.881389523 podStartE2EDuration="37.611339283s" podCreationTimestamp="2025-10-07 15:45:21 +0000 UTC" firstStartedPulling="2025-10-07 15:45:22.322795814 +0000 UTC m=+206.777583848" lastFinishedPulling="2025-10-07 15:45:48.052745574 +0000 UTC m=+232.507533608" observedRunningTime="2025-10-07 15:45:48.624781753 +0000 UTC m=+233.079569807" watchObservedRunningTime="2025-10-07 15:45:58.611339283 +0000 UTC m=+243.068645321"


==> storage-provisioner [490f5c8b02b9] <==
W1007 15:55:17.887678       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:17.945370       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:19.950238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:19.975000       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:21.978810       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:21.989891       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:23.995209       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:24.022689       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:26.027714       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:26.036787       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:28.042508       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:28.099862       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:30.106525       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:30.116629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:32.120342       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:32.136161       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:34.140405       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:34.150354       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:36.157494       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:36.171958       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:38.176592       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:38.206276       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:40.211433       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:40.233358       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:42.236997       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:42.276395       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:44.281684       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:44.312732       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:46.317000       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:46.330172       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:48.336059       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:48.348406       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:50.353530       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:50.412645       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:52.415758       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:52.424701       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:54.430798       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:54.443984       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:56.449502       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:56.493753       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:58.499030       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:55:58.513187       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:00.517756       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:00.546042       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:02.552034       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:02.574999       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:04.579073       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:04.615921       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:06.620374       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:06.646251       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:08.651861       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:08.671681       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:10.678907       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:10.693267       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:12.700053       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:12.714438       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:14.718796       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:14.727100       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:16.732827       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1007 15:56:16.743070       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [744c30c045ac] <==
I1007 15:42:02.544932       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1007 15:42:23.623189       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

